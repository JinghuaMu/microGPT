{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e72fb2-ffe7-4008-bfa0-2f65fed6a4a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Settings (empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4832388-12db-46b2-8730-026a3673e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import configparser\n",
    "# try:\n",
    "#    config = open(\"config.ini\")\n",
    "# except FileNotFoundError:\n",
    "#    API = input(\"Please Enter Open AI API:\")\n",
    "# else: \n",
    "#    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f36b5-8bb9-49b8-be19-b70c04b83832",
   "metadata": {
    "tags": []
   },
   "source": [
    "# API test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ab6f992-6039-444d-843e-511090d46f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import re\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1006d-e942-41e7-a1c2-1866b616186a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8ebbc194-1540-4b70-816b-73f89dbbe3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_abs(text,Require=\"\",question=\"\",usage=False):\n",
    "   default_require = \"Please answer my following questions in the most concise and precise manner possible, without answering complete sentences:\"\n",
    "   default_Qabs = '''\n",
    "         1. How many new strains have been discovered? Just return the number.\\n\n",
    "         2. What's the name of those new strains? Just return the names.\\n\n",
    "         3. What lineage does this species belong to? Use '-' to link different levels\\n\n",
    "         4. Where dose this strain come from? Just return the place.\\n \n",
    "         5. What the characteristics of these strains?\\n\n",
    "         6. What its best optimal growth environment?\\n\n",
    "         7. What is the metabolic profile of the strain?\\n\"\n",
    "   '''\n",
    "   if not question:\n",
    "      question = default_Qabs\n",
    "   if not Require:\n",
    "      Require = default_require\n",
    "   Prompt = \"{Require}:\\n{Abstract}\\nQuestions:\\n{Question}\".format(Require = Require, Abstract = text, Question = question)\n",
    "   response = openai.Completion.create(\n",
    "     model=\"text-davinci-003\",\n",
    "     prompt=Prompt,\n",
    "     temperature=0,\n",
    "     max_tokens=200,\n",
    "     top_p=1,\n",
    "     frequency_penalty=0,\n",
    "     presence_penalty=0\n",
    "   )\n",
    "   Answer = response['choices'][0]['text']\n",
    "   pattern = r\"\\d+\\.\\s+(.*)\"\n",
    "   Answer_list = re.findall(pattern,Answer)\n",
    "   Answer_list = [i.encode(\"utf8\") for i in Answer_list]\n",
    "   if usage:\n",
    "      print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens used.')\n",
    "   return Answer_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee71d7-7236-4afb-a035-4bce9329b8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bb0456d-1acf-45f4-804b-8eb90c8027ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Parse article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f48fc9cd-67f5-4e7c-b361-7dd8057880c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_meta_info(url,warning = False):\n",
    "   #Send HTTP request\n",
    "   headers = {\n",
    "       'User-Agent':\n",
    "       'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36',\n",
    "   }\n",
    "   try:\n",
    "      response = requests.get(url,headers,timeout=10)\n",
    "      if '404' in response.url:\n",
    "           raise Exception('No data found for this link: '+source_url)\n",
    "      Nopage = re.compile(r'Page Not Found')\n",
    "      if Nopage.search(response.text):\n",
    "         if warning:\n",
    "            raise Exception(\"Page Not Found\")\n",
    "\n",
    "      #Redirect the page\n",
    "      url_head = \"https://www.microbiologyresearch.org/\"\n",
    "      match_url = re.search(r'data-fullTexturl=\"(.*?)\"',response.text)\n",
    "      if match_url:\n",
    "         url_adhere = match_url.group(1)\n",
    "         redirect = url_head + url_adhere\n",
    "         full_text_url = requests.get(redirect,headers,timeout=10).text\n",
    "         full_text = requests.get(full_text_url,headers,timeout=10)\n",
    "      elif warning:\n",
    "         raise Exception(\"Can't Redirect\")\n",
    "   except:\n",
    "      raise\n",
    "\n",
    "   # Using latin encodeing and drop menu elements\n",
    "   soup = BeautifulSoup(full_text.text.encode('latin1').decode('utf-8').replace('\\u200a','').replace('\\u2006','').replace('\\u2009',''),features= 'html.parser')\n",
    "   elements_to_remove = soup.find_all('div', {'class': 'dropDownMenu'})\n",
    "   for element in elements_to_remove:\n",
    "       element.decompose()\n",
    "   elements_to_remove = soup.find_all('div', {'class': 'menuButton'})\n",
    "   for element in elements_to_remove:\n",
    "       element.decompose()\n",
    "\n",
    "   # get meta data\n",
    "   title = soup.find('h1').text.strip()\n",
    "   abstract = soup.find('div', {'class': 'articleSection article-abstract'}).text\n",
    "   abstract = re.sub(r'\\\\u[0-9a-fA-F]{4}', '',abstract)\n",
    "   article = soup.find_all('div', {'class': 'articleSection'})\n",
    "   return {\"title\":title,\"abstract\":abstract,\"article\":article}\n",
    "\n",
    "# Parsing full-text content\n",
    "def get_text(article):   \n",
    "   article_sorted ={\"Head\":\"\"}\n",
    "   regex_main = r'<a id=\".*?\" name=\".*?\">(.*?)</a><\\/div>'\n",
    "   delim_tag = r'<.*?>'\n",
    "   current_part = \"Head\"\n",
    "   for text in article:\n",
    "      main_match = re.search(regex_main, str(text))\n",
    "      if main_match:\n",
    "         sub_title = re.sub(delim_tag, '', main_match.group(1))\n",
    "         if sub_title != current_part:\n",
    "            article_sorted[sub_title]=\"\"\n",
    "            current_part = sub_title\n",
    "            for p in text.find_all('p'):\n",
    "                article_sorted[sub_title] += p.text\n",
    "      else: \n",
    "         for p in text.find_all('p'):\n",
    "                article_sorted[current_part] += p.text\n",
    "\n",
    "   for part in article_sorted:\n",
    "      article_sorted[part] = re.sub(r'\\[.*?\\]', '',article_sorted[part])\n",
    "      article_sorted[part] = re.sub(r'\\((http\\S+)\\)', '',article_sorted[part])\n",
    "      article_sorted[part] = re.sub(r'\\(([^)]*http[^)]*)\\)', '',article_sorted[part])\n",
    "      article_sorted[part] = re.sub(r'\\\\u[0-9a-fA-F]{4}', '',article_sorted[part])\n",
    "      \n",
    "   return article_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d753f6ba-249f-40e7-a7d0-02a71d07918a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# url = 'https://doi.org/10.1099/ijsem.0.004430'\n",
    "# get_text(get_meta_info(url)[\"article\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b5b86-70d1-4ce0-9db9-fcda2eb5ebef",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Parse volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb3c634-2d23-4c4c-9bac-59fc2aff97bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b3598fc-7502-4389-b6ca-0096162e2b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function return the newest catalog of IJSEM's volume, which is a list of url\n",
    "def get_volume_list():   \n",
    "   url_head = \"https://www.microbiologyresearch.org\"\n",
    "   all_volume = \"https://www.microbiologyresearch.org/content/journal/ijsem/issueslist\"\n",
    "   headers = {\n",
    "          'User-Agent':\n",
    "          'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36',\n",
    "      }\n",
    "   soup = BeautifulSoup(requests.get(all_volume,headers,timeout=10).text,'html.parser')\n",
    "   volume_list = []\n",
    "   grab_list = soup.find_all('a')\n",
    "   pattern = r'\"([^\"]*)\"'\n",
    "   for a in grab_list:\n",
    "      volume_list.append(url_head + re.search(pattern, str(a)).group(1))\n",
    "   return volume_list\n",
    "\n",
    "# The function return the soup of a volume page which will be used to get the volume info and article list\n",
    "def parse_volume(url):\n",
    "   headers = {\n",
    "          'User-Agent':\n",
    "          'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36',\n",
    "      }\n",
    "   url = url + \"?pageSize=100&page=1\"\n",
    "   volume_page = requests.get(url,headers,timeout=10)\n",
    "   soup = BeautifulSoup(volume_page.text)\n",
    "   return soup\n",
    "\n",
    "# The function return the volume info, which is a dictionary of title, date and year\n",
    "def get_volume_info(soup):\n",
    "   title = soup.find(\"meta\",  attrs={\"name\": \"citation_title\"})[\"content\"]\n",
    "   date = soup.find(\"meta\",  attrs={\"name\": \"citation_date\"})[\"content\"]\n",
    "   year = soup.find(\"meta\",  attrs={\"name\": \"citation_year\"})[\"content\"]\n",
    "   info = {\"title\":title,\"date\":date,\"year\":year}\n",
    "   return info\n",
    "\n",
    "# The function return the article list, which is a list of url\n",
    "def get_article_list(soup):\n",
    "   article_block = soup.find('div', {'class': 'issueTocContents table-wrapper'})\n",
    "   article_block = article_block.find_all('ul', {'class': 'list-unstyled'})\n",
    "   # Find the block of New Taxa\n",
    "   taxa_block_pos = -1\n",
    "   pattern = r'<span class=\"heading1\">\\nNew [Tt]axa\\n</span>'\n",
    "   for i in range(len(article_block)):\n",
    "      match = re.search(pattern,str(article_block[i]))\n",
    "      if match: taxa_block_pos = i\n",
    "   if taxa_block_pos == -1: return []\n",
    "   else:\n",
    "      taxa_block = str(article_block[taxa_block_pos])\n",
    "      \n",
    "   # Find the block of New Species in each region\n",
    "   pattern_field = r'(?<=<span class=\"tocheading2\">\\n)\\w+(?=\\n<\\/span>)'\n",
    "   taxa_list = [*re.finditer(pattern_field,taxa_block)]\n",
    "   pos = [i.start() for i in taxa_list]+[len(taxa_block)]\n",
    "   taxa = [i.group() for i in taxa_list]\n",
    "   article_list = {}\n",
    "   for i in range(len(taxa)):\n",
    "      sub_block = taxa_block[pos[i]:pos[i+1]]\n",
    "      pattern_doi = r'(?<=href=\")https://doi.org/10.1099/ijsem.0.\\d+'\n",
    "      article_list[taxa[i]] = re.findall(pattern_doi,sub_block)\n",
    "   return article_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe26f4a-17a7-40bb-bfbd-01d6a48c3f99",
   "metadata": {},
   "source": [
    "# Running test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba6988ef-4347-4158-8664-0f3dc2a77654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Little test\n",
    "\n",
    "# A = []\n",
    "# for i in get_article_list(parse_volume(get_volume_list()[1])).values():\n",
    "#    A = A+i\n",
    "\n",
    "# get_text(get_meta_info(A[3])[\"article\"])[\"Description of Muricauda spongiicola sp. nov.\"]\n",
    "# URL = \"https://doi.org/10.1099/ijsem.0.005691\"\n",
    "# a = get_meta_info(URL)[\"article\"]\n",
    "\n",
    "# text = ' <div class=\"articleSection\"><div class=\"sectionDivider\"><div class=\"tl-main-part title\"><a id=\"s9\" name=\"s9\">Description of <span class=\"jp-italic\">Acetomicrobiaceae</span> fam. nov.</a></div><div class=\"clearer\"> </div></div></div>'\n",
    "# regex_main = r'<a id=\".*?\" name=\".*?\">(.*?)</a><\\/div>'\n",
    "# main_match = re.search(regex_main, text)\n",
    "# print(main_match.group(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4d2167eb-b2d7-4487-b76f-7da10a1c6dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function return the meta info of each article and the specific answer of givin question\n",
    "# And save the data form one issue into a csv file\n",
    "# Description of parameters:\n",
    "# volume_range: an array of volume number that will be used to make the dataset\n",
    "# year_limit: the year limit of the dataset, 0 means no limit\n",
    "# require: the promote to chatGPT that will be used to make the dataset, blank means default\n",
    "# question: the question to chatGPT that will be used to make the dataset, blank means default\n",
    "# mode: the mode of the dataset, \"strain\" means each strain will be a sample, \n",
    "# \"article\" means each article will be a sample\n",
    "def establish_dataset(volume_range = [],year_limit=0,require=\"\",question=\"\",mode = \"strain\"):\n",
    "   import csv\n",
    "   import os \n",
    "   # get the list of volume\n",
    "   volume_list = get_volume_list()\n",
    "   # filter the volume list\n",
    "   if volume_range:\n",
    "      volume_list_neo = []\n",
    "      for i in volume_list:\n",
    "         if int(i.split('/')[-2]) in volume_range:\n",
    "            volume_list_neo.append(i)\n",
    "      volume_list = volume_list_neo\n",
    "   # get the info of each volume\n",
    "   for vol_url in volume_list:\n",
    "      try:\n",
    "         cur_vol = parse_volume(vol_url)\n",
    "         cur_vol_info = get_volume_info(cur_vol)\n",
    "         print(\"Current Volume: \",cur_vol_info[\"title\"])\n",
    "      except:\n",
    "         print(\"Read volume failed!\")\n",
    "         continue\n",
    "      # filter the volume by year and get the article list from each volume\n",
    "      if (int(cur_vol_info[\"year\"]) > year_limit):\n",
    "         cur_vol_article_list = get_article_list(cur_vol)\n",
    "         if not cur_vol_article_list:\n",
    "            print(\"No article found!\")\n",
    "            continue\n",
    "         # main loop of asking question to chatGPT\n",
    "         num_info = [print(i,\":\",len(j)) for i,j in cur_vol_article_list.items()]\n",
    "         span = sum([len(j) for j in cur_vol_article_list.values()])\n",
    "         cur_num = 0\n",
    "         # check if the file already exists\n",
    "         if not os.path.exists(cur_vol_info[\"title\"]+\".csv\"):\n",
    "            with open(cur_vol_info[\"title\"]+\".csv\", 'w', newline='', encoding = \"utf-8\") as csvfile:\n",
    "               writer = csv.writer(csvfile, delimiter='\\t')\n",
    "               for region, url_list in cur_vol_article_list.items():\n",
    "                  for article_url in url_list:\n",
    "                     cur_num += 1\n",
    "                     # get the meta info and text of each article\n",
    "                     try:\n",
    "                        if mode == \"strain\":\n",
    "                           meta_info = get_meta_info(article_url)\n",
    "                           text = get_text(meta_info[\"article\"])\n",
    "                        if mode == \"article\":\n",
    "                           meta_info = get_meta_info(article_url)\n",
    "                     except:\n",
    "                        print(\"Read failed!\")\n",
    "                        continue\n",
    "                     progess = \"({}/{})\".format(cur_num,span)\n",
    "                     print(progess+\"Currently parsing article:  \",meta_info[\"title\"])\n",
    "\n",
    "                     if mode == \"strain\":\n",
    "                        # get the strain info\n",
    "                        # the metainfos are: article_url, title, region, strain, answer\n",
    "                        count = 0\n",
    "                        for section, section_text in text.items():\n",
    "                           if re.search(\"[Dd]escription\",section):\n",
    "                              count += 1\n",
    "                              strain = section.split(\"of\")[-1].strip(\" \")\n",
    "                              answer = ask_abs(section_text,require,question)\n",
    "                              newline = [article_url.encode(\"utf8\")] + [meta_info[\"title\"].encode(\"utf8\")] + [region.encode(\"utf8\")] + [strain.encode(\"utf8\")] + answer\n",
    "                              writer.writerow(newline)\n",
    "                              csvfile.flush()\n",
    "                        if count == 0:\n",
    "                           print(\"No description found!\")\n",
    "                        else:\n",
    "                           print(\"Successfully written {} strains to the file!\".format(count))\n",
    "               \n",
    "                     if (mode == \"article\") or (mode == \"strain\" and count == 0):\n",
    "                        # get the abstract info \n",
    "                        # the metainfos are: article_url, title, region, strain, answer\n",
    "                        answer = ask_abs(meta_info[\"abstract\"],require,question)\n",
    "                        newline = [article_url.encode(\"utf8\")] + [meta_info[\"title\"].encode(\"utf8\")] + [region.encode(\"utf8\")] + [\"None\"] + answer\n",
    "                        writer.writerow(newline)\n",
    "                        csvfile.flush()\n",
    "                        if not mode == \"strain\":\n",
    "                           print(\"Successfully written to the file!\")\n",
    "                        else:\n",
    "                           print(\"Written info from the abstract\")\n",
    "         else:\n",
    "            print(\"File already exist!\")\n",
    "      else:\n",
    "         print(\"Volume reach the year limit!\")\n",
    "         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5240ac7b-68ba-4b20-8ff6-5842f5985ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Volume:  Volume 73, Issue 3\n",
      "Actinomycetota : 6\n",
      "Bacteroidota : 8\n",
      "Bacillota : 9\n",
      "Pseudomonadota : 13\n",
      "File already exist!\n",
      "Current Volume:  Volume 73, Issue 2\n",
      "Actinomycetota : 3\n",
      "Bacteroidota : 3\n",
      "Bacillota : 4\n",
      "Pseudomonadota : 19\n",
      "File already exist!\n",
      "Current Volume:  Volume 73, Issue 1\n",
      "Actinomycetota : 9\n",
      "Archaea : 1\n",
      "Bacteroidota : 2\n",
      "Bacillota : 9\n",
      "Pseudomonadota : 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Miniconda3\\envs\\bim\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/33)Currently parsing article:   Streptomyces rhizoryzae sp. nov., isolated from paddy rhizosphere soil and formal proposal to reclassify Streptomyces albulus as a later heterotypic synonym of Streptomyces noursei\n",
      "Successfully written 1 strains to the file!\n",
      "(2/33)Currently parsing article:   Frankia colletiae sp. nov., a nitrogen-fixing actinobacterium isolated from Colletia cruciata\n",
      "Successfully written 1 strains to the file!\n",
      "(3/33)Currently parsing article:   Description of Jidongwangia harbinensis gen. nov. sp. nov\n",
      "Successfully written 2 strains to the file!\n",
      "(4/33)Currently parsing article:   Pseudarthrobacter humi sp. nov., an actinobacterium isolated from soil\n",
      "Successfully written 1 strains to the file!\n",
      "(5/33)Currently parsing article:   Solicola gregarius gen. nov., sp. nov., a soil actinobacterium isolated after enhanced cultivation with Micrococcus luteus culture supernatant\n",
      "No description found!\n",
      "(6/33)Currently parsing article:   Streptomyces guryensis sp. nov. exhibiting antimicrobial activity, isolated from riverside soil\n",
      "Successfully written 1 strains to the file!\n",
      "(7/33)Currently parsing article:   Streptomyces macrolidinus sp. nov., a novel soil actinobacterium with potential anticancer and antimalarial activity\n",
      "Successfully written 1 strains to the file!\n",
      "(8/33)Currently parsing article:   Actinomadura terrae sp. nov. and Actinomadura litoris OS3-89, isolated from rhizosphere soil of cactus\n",
      "Successfully written 1 strains to the file!\n",
      "(9/33)Currently parsing article:   Agromyces seonyuensis sp. nov., isolated from island soil\n",
      "Successfully written 1 strains to the file!\n",
      "(10/33)Currently parsing article:   Methanocaldococcus lauensis sp. nov., a novel deep-sea hydrothermal vent hyperthermophilic methanogen\n",
      "Successfully written 1 strains to the file!\n",
      "(11/33)Currently parsing article:   Flavihumibacter fluminis sp. nov., a novel thermotolerant bacterium isolated from river silt\n",
      "Successfully written 1 strains to the file!\n",
      "(12/33)Currently parsing article:   Bacteroides faecium sp. nov. isolated from human faeces\n",
      "No description found!\n",
      "(13/33)Currently parsing article:   Allobaculum mucilyticum sp. nov. and Allobaculum fili sp. nov., isolated from the human intestinal tract\n",
      "Successfully written 2 strains to the file!\n",
      "(14/33)Currently parsing article:   Terrisporobacter hibernicus sp. nov., isolated from bovine faeces in Northern Ireland\n",
      "Successfully written 1 strains to the file!\n",
      "(15/33)Currently parsing article:   Lutispora saccharofermentans sp. nov., a mesophilic, non-spore-forming bacterium isolated from a lab-scale methanogenic landfill bioreactor digesting anaerobic sludge, and emendation of the genus Lutispora to include species which are non-spore-forming and mesophilic\n",
      "Successfully written 2 strains to the file!\n",
      "(16/33)Currently parsing article:   Clostridium caldaquaticum sp. nov., a thermophilic bacterium isolated from a hot spring sediment\n",
      "Successfully written 1 strains to the file!\n",
      "(17/33)Currently parsing article:   Reclassification of Clostridium cocleatum, Clostridium ramosum, Clostridium spiroforme and Clostridium saccharogumia as Thomasclavelia cocleata gen. nov., comb. nov., Thomasclavelia ramosa comb. nov., gen. nov., Thomasclavelia spiroformis comb. nov. and Thomasclavelia saccharogumia comb. nov\n",
      "No description found!\n",
      "(18/33)Currently parsing article:   Anaeromicropila herbilytica gen. nov., sp. nov., a plant polysaccharide-decomposing anaerobic bacterium isolated from anoxic soil subjected to reductive soil disinfestation, and reclassification of Clostridium populeti as Anaeromicropila populeti comb. nov.\n",
      "Successfully written 3 strains to the file!\n",
      "(19/33)Currently parsing article:   Paenibacillus rhizolycopersici sp. nov., an oligotrophic bacterium isolated from a tomato plant in China\n",
      "Successfully written 1 strains to the file!\n",
      "(20/33)Currently parsing article:   Thermospira aquatica gen. nov., sp. nov., a novel thermophilic spirochete isolated from a Tunisian hot spring, and description of the novel family Thermospiraceae.\n",
      "Successfully written 3 strains to the file!\n",
      "(21/33)Currently parsing article:   Akkermansia biwaensis sp. nov., an anaerobic mucin-degrading bacterium isolated from human faeces\n",
      "Successfully written 1 strains to the file!\n",
      "(22/33)Currently parsing article:   Pseudomonas paralcaligenes sp. nov., isolated from a hospitalized patient\n",
      "Successfully written 1 strains to the file!\n",
      "(23/33)Currently parsing article:   Octadecabacter algicola sp. nov. and Octadecabacter dasysiphoniae sp. nov., isolated from a marine red alga and emended description of the genus Octadecabacter\n",
      "Successfully written 3 strains to the file!\n",
      "(24/33)Currently parsing article:   Phylogenomic analysis of the genus Alcanivorax: proposal for division of this genus into the emended genus Alcanivorax and two novel genera Alloalcanivorax gen. nov. and Isoalcanivorax gen. nov.\n",
      "Successfully written 1 strains to the file!\n",
      "(25/33)Currently parsing article:   Zeimonas sediminis sp. nov., isolated from mangrove sediment\n",
      "Successfully written 1 strains to the file!\n",
      "(26/33)Currently parsing article:   Vibrio amylolyticus sp. nov. and Vibrio gelatinilyticus sp. nov., two marine bacteria isolated from surface seawater of Qingdao\n",
      "Successfully written 2 strains to the file!\n",
      "(27/33)Currently parsing article:   Vibrio sinus sp. nov., a marine bacterium isolated from coastal seawater\n",
      "Successfully written 1 strains to the file!\n",
      "(28/33)Currently parsing article:   Maritalea mediterranea sp. nov., isolated from marine plastic residues\n",
      "Successfully written 1 strains to the file!\n",
      "(29/33)Currently parsing article:   Legionella maioricensis sp. nov., a new species isolated from the hot water distribution systems of a hospital and a shopping center during routine sampling\n",
      "Successfully written 1 strains to the file!\n",
      "(30/33)Currently parsing article:   Sandaracinobacteroides sayramensis sp. nov., a yellow-pigmented bacterium isolated from lake water\n",
      "Successfully written 1 strains to the file!\n",
      "(31/33)Currently parsing article:   Melaminivora suipulveris sp. nov., isolated from pigpen dust\n",
      "Successfully written 1 strains to the file!\n",
      "(32/33)Currently parsing article:   Morphological and genetic analysis of Ceratomyxa saurida Zhao et al. 2015 and Ceratomyxa mai sp. nov. (Myxozoa: Ceratomyxidae) from the East China Sea\n",
      "No description found!\n",
      "(33/33)Currently parsing article:   Proposal of Holzapfeliella gen. nov. and Litorivicinus gen. nov. as replacement names for the illegitimate prokaryotic generic names Holzapfelia Zheng et al. 2020 and Litoricola Kim et al. 2007, respectively\n",
      "Successfully written 6 strains to the file!\n"
     ]
    }
   ],
   "source": [
    "r1 = \"Please complete the setences by filling the blank in brackets:\"\n",
    "q1 = '''\n",
    "         1. There are () number of new strains was found.\\n\n",
    "         2. The name of those strains are ().\\n\n",
    "         3. The full lineage of the strain is ().\\n\n",
    "         4. The strain was extracted from ().\\n \n",
    "         5. The characteristic of the strain is ().\\n\n",
    "         6. The best optimal growth environment is ().\\n\n",
    "         7. The metabolic profile of the strain is ().\\n\"\n",
    "   '''\n",
    "\n",
    "establish_dataset([73],require=r1,question=q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d07c0f0d-7709-4517-ad80-1235e73aff10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
